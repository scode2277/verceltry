name: Process and Upload Images

# ============================================================================
# WORKFLOW PURPOSE
# ============================================================================
# This workflow is designed for the following contribution flow:
#
# 1. Contributors fork the repository and make changes
# 2. They open a PR against the Frameworks' 'develop' branch
# 3. They add images directly to the PR description or PR comments
#    (GitHub automatically hosts these images and provides URLs)
# 4. A maintainer reviews the PR and the images (visible inline)
# 5. Maintainer comments '/approve-images' followed by the GitHub image URLs
#    (This explicit approval ensures only verified URLs are processed)
# 6. This workflow processes those URLs and uploads images to AWS S3
# 7. The workflow posts results back to the PR with permanent S3 URLs
#
# Benefits of this approach:
# - Contributors can easily paste images directly in PRs (no external uploads)
# - Images are visible inline during PR review
# - Maintainer explicitly approves each URL for security
# - GitHub URLs are permanent and reliable
# - No dependency on external hosting services
# ============================================================================

# This workflow triggers when a comment is created on an issue or pull request
# It processes image URLs from comments and uploads them to AWS S3
on:
  issue_comment:
    types: [created]

permissions:
  pull-requests: write
  issues: write
  contents: read

jobs:
  upload-images:
    # Concurrency control: Organize workflow runs by PR number
    concurrency:
      group: upload-images-${{ github.event.issue.number }}
      cancel-in-progress: false
    
    # Conditional execution: Only run if ALL conditions are met:
    # 1. Comment is on a pull request (not a regular issue)
    # 2. Comment starts with '/approve-images' command
    # 3. Commenter has appropriate permissions (OWNER, MEMBER, or COLLABORATOR)
    if: |
      github.event.issue.pull_request &&
      startsWith(github.event.comment.body, '/approve-images') &&
      (github.event.comment.author_association == 'OWNER' || 
       github.event.comment.author_association == 'MEMBER' ||
       github.event.comment.author_association == 'COLLABORATOR')
    
    runs-on: ubuntu-latest
    
    steps:
      # Step 1: Acknowledge the command by adding a reaction
      - name: React to command
        uses: actions/github-script@v7
        with:
          script: |
            // Add an "eyes" emoji reaction to the comment to show we're processing
            github.rest.reactions.createForIssueComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: context.payload.comment.id,
              content: 'eyes'
            });

      # Step 2: Extract GitHub image URLs from the comment
      - name: Extract GitHub image URLs
        id: extract-urls
        uses: actions/github-script@v7
        with:
          script: |
            const comment = context.payload.comment.body;

            // Extract URLs from comment using regex
            const urlRegex = /(https:\/\/[^\s]+)/g;
            const urls = comment.match(urlRegex) || [];

            const validatedUrls = [];
            const validationErrors = [];

            for (const url of urls) {
              if (url === '/approve-images') continue;

              try {
                const urlObj = new URL(url);

                // Only allow HTTPS (GitHub always uses HTTPS)
                if (urlObj.protocol !== 'https:') {
                  validationErrors.push(`${url}: Only HTTPS URLs are allowed`);
                  continue;
                }

                //we could add a check that after this hostname, there is a / and 9 num hash and then again / and after 46 char there is .png

                // Ensure it's a valid GitHub-hosted image
                const hostname = urlObj.hostname.toLowerCase();
                const isValidGitHubHost =
                  hostname.includes('user-images.githubusercontent.com') ||
                  hostname.includes('private-user-images.githubusercontent.com') ||
                  (hostname.includes('github.com') && url.includes('/assets/'));

                if (!isValidGitHubHost) {
                  validationErrors.push(`${url}: Not a valid GitHub image URL`);
                  continue;
                }

                validatedUrls.push(url);
              } catch (error) {
                validationErrors.push(`${url}: Invalid URL format - ${error.message}`);
              }
            }

            if (validatedUrls.length === 0 && validationErrors.length === 0) {
              throw new Error('No GitHub-hosted image URLs found in comment.');
            }

            if (validationErrors.length > 0) {
              throw new Error(
                `URL validation failed:\n${validationErrors.join('\n')}\n\n` +
                `Please ensure all URLs are GitHub-hosted image URLs.`
              );
            }

            console.log(`Extracted ${validatedUrls.length} GitHub image URL(s)`);
            console.log(
              'URLs:',
              validatedUrls.map(u => {
                const truncated = u.length > 60 ? u.substring(0, 60) + '...' : u;
                return truncated;
              })
            );

            // Return validated URLs as JSON array
            return validatedUrls;
          result-encoding: json

      # Step 3: Validate that all required AWS secrets are configured
      - name: Validate AWS secrets
        run: |
          if [ -z "${{ secrets.AWS_ACCESS_KEY_ID }}" ]; then
            echo "::error::AWS_ACCESS_KEY_ID secret is not configured"
            exit 1
          fi
          
          if [ -z "${{ secrets.AWS_SECRET_ACCESS_KEY }}" ]; then
            echo "::error::AWS_SECRET_ACCESS_KEY secret is not configured"
            exit 1
          fi
          
          if [ -z "${{ secrets.AWS_REGION }}" ]; then
            echo "::error::AWS_REGION secret is not configured"
            exit 1
          fi
          
          if [ -z "${{ secrets.AWS_S3_BUCKET }}" ]; then
            echo "::error::AWS_S3_BUCKET secret is not configured"
            exit 1
          fi
          
          echo "‚úì All AWS secrets are configured"

      # Step 4: Checkout repository
      - name: Checkout repository
        uses: actions/checkout@v4

      # Step 5: Setup Node.js and pnpm
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
      
      # Step 6: Install pnpm
      - name: Setup pnpm
        uses: pnpm/action-setup@v4
        with:
          version: 10.15.0 
          run_install: false

      # Step 7: Cache pnpm dependencies
      - name: Cache pnpm dependencies
        uses: actions/cache@v4
        id: pnpm-cache
        with:
          path: |
            node_modules
            ~/.pnpm-store
          key: ${{ runner.os }}-pnpm-${{ hashFiles('**/pnpm-lock.yaml') }}
          restore-keys: |
            ${{ runner.os }}-pnpm-

      # Step 8: Install dependencies
      - name: Install dependencies
        run: |
          # Install dependencies using pnpm
          # pnpm will automatically use the cache if available
          # Using --no-frozen-lockfile because pnpm-lock.yaml may not exist yet
          # TODO: After running 'pnpm install' locally and committing pnpm-lock.yaml,
          # change this back to 'pnpm install --frozen-lockfile' for better reproducibility
          pnpm install --no-frozen-lockfile

      # Step 9: Write URLs to file for the processing script
      - name: Write URLs to file
        uses: actions/github-script@v7
        env:
          URLS_JSON: ${{ steps.extract-urls.outputs.result }}
        with:
          script: |
            const fs = require('fs');

            const urls = JSON.parse(process.env.URLS_JSON);

            fs.writeFileSync('urls.json', JSON.stringify(urls, null, 2));
            
            console.log(`Wrote ${urls.length} URL(s) to urls.json`);

      # Step 10: Process and upload images
      - name: Process and upload images
        id: process
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
          AWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          node .github/scripts/process-images.js > output.log 2>&1
          
          echo "Extracting RESULTS from output.log..."
          
          RESULTS=$(perl -0777 -pe 's/.*RESULTS:\s*//s' output.log 2>/dev/null || echo ''
          
          RESULTS=$(echo "$RESULTS" | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')
          
          if [ -z "$RESULTS" ] || ! echo "$RESULTS" | grep -q '^\['; then
            echo "‚ö† Warning: No valid RESULTS found or invalid format"
            echo "Preview of output.log tail:"
            tail -20 output.log
            RESULTS='[]'
          fi
          
          echo "$RESULTS" > results.json
          
          if [ -f results.json ]; then
            FILE_SIZE=$(wc -c < results.json | tr -d ' ')
            echo "‚úì results.json created: $FILE_SIZE bytes"
            echo "Preview: $(head -c 200 results.json)..."
            
            if command -v jq &> /dev/null; then
              if jq empty results.json 2>/dev/null; then
                echo "‚úì results.json contains valid JSON"
              else
                echo "‚ö† Warning: results.json may not be valid JSON"
              fi
            fi
          else
            echo "‚ùå Error: results.json was not created!"
            exit 1
          fi
 
          cat output.log

      # Step 11: Comment results back to PR and check for failures
      - name: Comment results and check failures
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            let results;
            try {
              const resultsPath = 'results.json';
              if (!fs.existsSync(resultsPath)) {
                throw new Error('Results file not found - workflow may have failed before processing');
              }
              
              const resultsStr = fs.readFileSync(resultsPath, 'utf8').trim();
              if (!resultsStr || resultsStr === '') {
                throw new Error('Results file is empty - workflow may have failed before processing');
              }
              
              results = JSON.parse(resultsStr);
              
              if (!Array.isArray(results)) {
                throw new Error(`Results is not an array: ${typeof results}`);
              }
              
              if (results.length === 0) {
                throw new Error('No results found in results array');
              }
            } catch (error) {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.payload.issue.number,
                body: `‚ùå **Error**: Failed to parse workflow results: ${error.message}\n\nPlease check the workflow logs for details.`
              });
              throw error;
            }
            
            function sanitizeUrl(url) {
              if (!url) return '';
              return url.replace(/[<>]/g, '');
            }

            // Build comment body with results
            let commentBody = '## üñºÔ∏è Image Upload Results\n\n';
            commentBody += '**Next steps**: Copy the permanent S3 URLs below and replace the temporary URLs in the PR files.\n\n';
            
            // Separate successful and failed uploads
            const successful = results.filter(r => r.success);
            const failed = results.filter(r => !r.success);
            
            // Add successful uploads section
            if (successful.length > 0) {
              commentBody += '### ‚úÖ Successfully Uploaded\n\n';
              successful.forEach(result => {
                const safeOriginalUrl = sanitizeUrl(result.originalUrl);
                const safeS3Url = sanitizeUrl(result.s3Url);
                
                commentBody += `#### ${result.filename}\n`;
                commentBody += `- **Original URL**: ${safeOriginalUrl}\n`;
                commentBody += `- **S3 URL**: ${safeS3Url}\n`;
                commentBody += '---\n\n';
              });
            }
            
            // Add failed uploads section
            if (failed.length > 0) {
              commentBody += '### ‚ùå Failed Uploads\n\n';
              failed.forEach(result => {
                const safeUrl = sanitizeUrl(result.originalUrl);
                commentBody += `- **URL**: ${safeUrl}\n`;
                commentBody += `- **Error**: ${result.error}\n\n`;
              });
            }
            
            // Add summary
            commentBody += `\n*Processed ${results.length} image(s): ${successful.length} successful, ${failed.length} failed*`;
            
            // Post comment to PR
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.payload.issue.number,
              body: commentBody
            });
            
            // Add reaction to original comment to indicate completion
            await github.rest.reactions.createForIssueComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: context.payload.comment.id,
              content: successful.length === results.length ? 'hooray' : ''
            });

      # Step 12: Check for failures and fail workflow if needed
      - name: Check for failures
        run: |
          # Read results from file (written by processing step)
          if [ ! -f results.json ]; then
            echo "::error::Results file not found"
            exit 1
          fi
          
          # Count failed uploads using jq (JSON processor)
          FAILED=$(jq '[.[] | select(.success == false)] | length' results.json)
          
          # If any uploads failed, exit with error
          if [ "$FAILED" -gt 0 ]; then
            echo "::error::$FAILED image(s) failed to upload"
            exit 1
          fi
          
          echo "‚úì All images uploaded successfully!"

      # Step 13: Cleanup temporary files
      - name: Cleanup temporary files
        if: always()
        run: |
          # Remove temporary files (but not the script file from .github/scripts/)
          rm -f urls.json output.log results.json
          echo "‚úì Cleaned up temporary files"

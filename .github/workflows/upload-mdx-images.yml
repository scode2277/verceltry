name: Process and Upload Images

on:
  issue_comment:
    types: [created]

# Prevents workflow from running on forks - security critical!
# Only runs when comment is on a PR in the main repo
permissions:
  pull-requests: write
  issues: write
  contents: read

jobs:
  upload-images:
    # Only run if:
    # 1. Comment is on a pull request (not an issue)
    # 2. Comment starts with /approve-images
    # 3. Commenter has write permission (is a maintainer)
    if: |
      github.event.issue.pull_request &&
      startsWith(github.event.comment.body, '/approve-images') &&
      (github.event.comment.author_association == 'OWNER' || 
       github.event.comment.author_association == 'MEMBER' ||
       github.event.comment.author_association == 'COLLABORATOR')
    
    runs-on: ubuntu-latest
    
    steps:
      # Step 1: Acknowledge the command
      - name: React to command
        uses: actions/github-script@v7
        with:
          script: |
            github.rest.reactions.createForIssueComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: context.payload.comment.id,
              content: 'eyes'
            });

      # Step 2: Extract image URLs from the comment
      - name: Extract image URLs
        id: extract-urls
        uses: actions/github-script@v7
        with:
          script: |
            const comment = context.payload.comment.body;
            
            // Extract URLs from comment (supports multiple formats)
            // Matches: http://, https://, www.
            const urlRegex = /(https?:\/\/[^\s]+)/g;
            const urls = comment.match(urlRegex) || [];
            
            // Filter out the /approve-images command itself
            const imageUrls = urls.filter(url => 
              !url.includes('github.com') && 
              url !== '/approve-images'
            );
            
            if (imageUrls.length === 0) {
              throw new Error('No image URLs found in comment. Please provide URLs after /approve-images');
            }
            
            console.log('Found URLs:', imageUrls);
            
            // Save URLs for next steps
            return imageUrls;
          result-encoding: json

      # Step 3: Setup Node.js environment
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      # Step 4: Install dependencies
      - name: Install dependencies
        run: |
          npm install @aws-sdk/client-s3 @aws-sdk/lib-storage axios sharp uuid

      # Step 5: Create the image processing script
      - name: Create processing script
        run: |
          cat > process-images.js << 'SCRIPT_EOF'
          const { S3Client } = require('@aws-sdk/client-s3');
          const { Upload } = require('@aws-sdk/lib-storage');
          const axios = require('axios');
          const sharp = require('sharp');
          const { v4: uuidv4 } = require('uuid');
          const crypto = require('crypto');
          const path = require('path');

          // Configuration
          const MAX_FILE_SIZE = 10 * 1024 * 1024; // 10MB
          const MAX_WIDTH = 4096;
          const MAX_HEIGHT = 4096;
          const ALLOWED_FORMATS = ['jpeg', 'jpg', 'png', 'gif', 'webp', 'svg'];

          // Initialize S3 client
          const s3Client = new S3Client({
            region: process.env.AWS_REGION,
            credentials: {
              accessKeyId: process.env.AWS_ACCESS_KEY_ID,
              secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY
            }
          });

          async function downloadImage(url) {
            console.log(`Downloading: ${url}`);
            
            const response = await axios({
              url,
              method: 'GET',
              responseType: 'arraybuffer',
              maxContentLength: MAX_FILE_SIZE,
              timeout: 30000,
              headers: {
                'User-Agent': 'GitHub-Image-Upload-Bot/1.0'
              }
            });

            return Buffer.from(response.data);
          }

          async function validateImage(buffer, originalUrl) {
            console.log('Validating image...');

            // Check file size
            if (buffer.length > MAX_FILE_SIZE) {
              throw new Error(`Image exceeds ${MAX_FILE_SIZE / 1024 / 1024}MB size limit`);
            }

            // Use sharp to validate and get metadata
            try {
              const metadata = await sharp(buffer).metadata();
              
              console.log('Image metadata:', {
                format: metadata.format,
                width: metadata.width,
                height: metadata.height,
                size: `${(buffer.length / 1024).toFixed(2)}KB`
              });

              // Check format
              if (!ALLOWED_FORMATS.includes(metadata.format)) {
                throw new Error(`Invalid format: ${metadata.format}. Allowed: ${ALLOWED_FORMATS.join(', ')}`);
              }

              // Check dimensions
              if (metadata.width > MAX_WIDTH || metadata.height > MAX_HEIGHT) {
                throw new Error(`Image dimensions ${metadata.width}x${metadata.height} exceed maximum ${MAX_WIDTH}x${MAX_HEIGHT}`);
              }

              // Basic malware check: ensure it's actually an image
              // Sharp will throw an error if the file is not a valid image
              await sharp(buffer).toBuffer();

              return metadata;
            } catch (error) {
              throw new Error(`Image validation failed: ${error.message}`);
            }
          }

          function generateUniqueFilename(originalUrl, format) {
            const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
            const uniqueId = uuidv4().split('-')[0]; // First segment of UUID
            const hash = crypto.createHash('md5').update(originalUrl).digest('hex').substring(0, 8);
            
            // Try to preserve original filename if possible
            let originalName = 'image';
            try {
              const urlPath = new URL(originalUrl).pathname;
              const basename = path.basename(urlPath, path.extname(urlPath));
              if (basename && basename !== '' && basename.length < 50) {
                originalName = basename.replace(/[^a-zA-Z0-9-_]/g, '-');
              }
            } catch (e) {
              // Use default if URL parsing fails
            }

            return `${timestamp}_${uniqueId}_${hash}_${originalName}.${format}`;
          }

          async function uploadToS3(buffer, filename, contentType) {
            console.log(`Uploading to S3: ${filename}`);

            const upload = new Upload({
              client: s3Client,
              params: {
                Bucket: process.env.AWS_S3_BUCKET,
                Key: `images/${filename}`,
                Body: buffer,
                ContentType: contentType,
                CacheControl: 'public, max-age=31536000', // 1 year cache
                ServerSideEncryption: 'AES256'
              }
            });

            await upload.done();

            // Generate public URL
            const region = process.env.AWS_REGION;
            const bucket = process.env.AWS_S3_BUCKET;
            const publicUrl = `https://${bucket}.s3.${region}.amazonaws.com/images/${filename}`;
            
            return publicUrl;
          }

          async function processImage(url) {
            try {
              // Download
              const buffer = await downloadImage(url);
              
              // Validate and get metadata
              const metadata = await validateImage(buffer, url);
              
              // Generate unique filename
              const filename = generateUniqueFilename(url, metadata.format);
              
              // Upload to S3
              const s3Url = await uploadToS3(
                buffer, 
                filename, 
                `image/${metadata.format}`
              );

              return {
                success: true,
                originalUrl: url,
                s3Url: s3Url,
                filename: filename,
                metadata: {
                  format: metadata.format,
                  width: metadata.width,
                  height: metadata.height,
                  size: `${(buffer.length / 1024).toFixed(2)}KB`
                }
              };
            } catch (error) {
              return {
                success: false,
                originalUrl: url,
                error: error.message
              };
            }
          }

          async function main() {
            // Read URLs from file instead of env var
            const fs = require('fs');
            let urls;
            
            try {
              const rawData = fs.readFileSync('urls.json', 'utf8');
              console.log('Raw urls.json:', rawData);
              
              urls = JSON.parse(rawData);
              
              // Ensure it's an array
              if (!Array.isArray(urls)) {
                throw new Error('urls.json does not contain an array');
              }
              
              console.log(`Processing ${urls.length} image(s)...`);
              console.log('URLs to process:', urls);
            } catch (error) {
              console.error('Failed to read/parse urls.json:', error.message);
              process.exit(1);
            }

            const results = [];
            for (const url of urls) {
              const result = await processImage(url);
              results.push(result);
            }

            // Output results as JSON for GitHub Actions
            console.log('RESULTS:', JSON.stringify(results));
          }

          main().catch(error => {
            console.error('Fatal error:', error);
            process.exit(1);
          });
          SCRIPT_EOF

      # Step 6: Process and upload images
      - name: Process and upload images
        id: process
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
          AWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}
        run: |
          # Write URLs to file to avoid env var encoding issues
          echo '${{ toJSON(steps.extract-urls.outputs.result) }}' > urls.json
          
          node process-images.js > output.log 2>&1
          
          # Extract results from log
          RESULTS=$(grep "RESULTS:" output.log | sed 's/RESULTS: //')
          echo "results=$RESULTS" >> $GITHUB_OUTPUT
          
          # Show full log for debugging
          cat output.log

      # Step 7: Comment results back to PR
      - name: Comment results
        uses: actions/github-script@v7
        with:
          script: |
            const results = JSON.parse('${{ steps.process.outputs.results }}');
            
            let commentBody = '## ðŸ–¼ï¸ Image Upload Results\n\n';
            
            const successful = results.filter(r => r.success);
            const failed = results.filter(r => !r.success);
            
            if (successful.length > 0) {
              commentBody += '### âœ… Successfully Uploaded\n\n';
              successful.forEach(result => {
                commentBody += `#### ${result.filename}\n`;
                commentBody += `- **Original URL**: ${result.originalUrl}\n`;
                commentBody += `- **S3 URL**: ${result.s3Url}\n`;
                commentBody += `- **Format**: ${result.metadata.format}\n`;
                commentBody += `- **Dimensions**: ${result.metadata.width}Ã—${result.metadata.height}\n`;
                commentBody += `- **Size**: ${result.metadata.size}\n\n`;
                commentBody += `![Preview](${result.s3Url})\n\n`;
                commentBody += '---\n\n';
              });
            }
            
            if (failed.length > 0) {
              commentBody += '### âŒ Failed Uploads\n\n';
              failed.forEach(result => {
                commentBody += `- **URL**: ${result.originalUrl}\n`;
                commentBody += `- **Error**: ${result.error}\n\n`;
              });
            }
            
            commentBody += `\n*Processed ${results.length} image(s): ${successful.length} successful, ${failed.length} failed*`;
            
            // Post comment
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.payload.issue.number,
              body: commentBody
            });
            
            // Add reaction to indicate completion
            await github.rest.reactions.createForIssueComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: context.payload.comment.id,
              content: successful.length === results.length ? 'rocket' : 'confused'
            });

      # Step 8: Fail workflow if any upload failed
      - name: Check for failures
        run: |
          RESULTS='${{ steps.process.outputs.results }}'
          FAILED=$(echo "$RESULTS" | jq '[.[] | select(.success == false)] | length')
          
          if [ "$FAILED" -gt 0 ]; then
            echo "::error::$FAILED image(s) failed to upload"
            exit 1
          fi
          
          echo "All images uploaded successfully!"